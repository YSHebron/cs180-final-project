{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Air Quality and COVID-19 Infection Rates\n",
    "For this project, our goal is to investigate a possible relationship between Air Quality and COVID-19 Infection Rates. To that end, we will merge an Air Quality dataset with a COVID-19 dataset, train a linear regression model on the merged dataset, and perform predictions to test the accuracy of our model.\n",
    "\n",
    "Some thoughts about the web app:\n",
    "- Perhaps an app where Air Quality features and COVID-19 cases are the independent variables while COVID-19 infection rate (%) is the dependent variable.\n",
    "- Sliders and input fields would be used for the independent variables.\n",
    "\n",
    "**Important:** Because this is an exploratory project, we are not absolutely certain that there exists a significant relationship between Air Quality and COVID-19 datasets. Furthermore, our results, significant or not, can only lend support to hypotheses surrounding Air Quality and COVID-19.\n",
    "\n",
    "For full project details, please see `CS180_G16_ProjectProposal.pdf`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Examining the Datasets\n",
    "Let us look at the samples, features, and other details regarding the dataset so that we know what we're dealing with."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "def newline(): print(\"---------------------------\\n\")\n",
    "\n",
    "import pandas as pd\n",
    "dataA = pd.read_csv('datasets/A-WHO-air-quality.csv')\n",
    "dataB = pd.read_csv('datasets/B-WHO-covid-infections-deaths.csv')\n",
    "\n",
    "print(\"Dataset A: WHO Air Quality (2023)\")\n",
    "print(\"Dataset A Contents\")\n",
    "dataA.head()\n",
    "print(\"Additional Details\")\n",
    "dataA.describe()\n",
    "dataA.info()\n",
    "\n",
    "newline()\n",
    "\n",
    "print(\"Dataset B: WHO COVID-19 Cases and Deaths (2023)\")\n",
    "print(\"Dataset B Contents\")\n",
    "dataB.head()\n",
    "print(\"Additional Details\")\n",
    "dataB.describe()\n",
    "dataB.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First Dropping of Features\n",
    "In this section, we are going to drop the features that are obviously unrelated to our purposes.\n",
    "\n",
    "Looking at Dataset A's features (and samples for confirmation), we find that the following features are unnecessary in our context:\n",
    "\n",
    "- who_region\n",
    "- iso3\n",
    "- city (because there's no city in Dataset B)\n",
    "- version\n",
    "- type_of_stations\n",
    "- reference\n",
    "- web_link\n",
    "- population (also has to be dropped because this is by [country, city])\n",
    "- population_source\n",
    "- latitude (because analysis is on a per country basis)\n",
    "- longitude (ditto)\n",
    "- who_ms\n",
    "\n",
    "Similarly, for Dataset B, the following are unnecessary:\n",
    "\n",
    "- Country_code\n",
    "- WHO_region\n",
    "\n",
    "Note that explicitly printing the labels shows that there's no abnormality with their names. Thanks WHO. But I think we should remove the capitalizations in Dataset B's labels to match Dataset A.\n",
    "\n",
    "We shall now drop these features:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dropping columns from Dataset A\n",
    "dataA.drop(['who_region', 'iso3', 'city', 'version', 'type_of_stations',\n",
    "            'reference', 'web_link', 'population', 'population_source',\n",
    "            'latitude', 'longitude', 'who_ms'], axis=1, inplace=True)\n",
    "# Dropping columns from Dataset B\n",
    "dataB.drop(['Country_code', 'WHO_region'], axis=1, inplace=True)\n",
    "\n",
    "# Converting all of Dataset B's features to lowercase\n",
    "newLabels = []\n",
    "for label in dataB.columns:\n",
    "    newLabels.append(label.lower())\n",
    "dataB.columns = newLabels\n",
    "\n",
    "# Also convert country_name label to country\n",
    "dataA.rename(columns={'country_name':'country'},inplace=True)\n",
    "\n",
    "# Checking Dataset A and B state\n",
    "print(\"Dataset A\")\n",
    "dataA.head()\n",
    "print(\"Dataset B\")\n",
    "dataB.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Temporal Restriction\n",
    "As mentioned in our project proposal, we would limit our project to records from the year 2020-2022 to capture only the records which the COVID-19 pandemic may have impacted.\n",
    "\n",
    "*Remark:* We're dropping records from beyond 2022 from Dataset B because Dataset A is restricted to until 2022.\n",
    "\n",
    "Because of the way the date_reported in dataset B is formatted (date object with format YYYY-MM-DD), we cannot use it directly for comparison with the year in Dataset A of format YYYY. We must first extract year from the date_reported and replace that column with year."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Replaced date_reported with Year\n",
    "dataB['date_reported'] = pd.to_datetime(dataB['date_reported'])\n",
    "dataB['year'] = dataB['date_reported'].dt.year\n",
    "del dataB['date_reported']\n",
    "cols = dataB.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "dataB = dataB[cols]\n",
    "print('Dataset B')\n",
    "dataB"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Aux: Sort dataset by country name\n",
    "dataA.sort_values('country', ascending=True, inplace=True)\n",
    "dataA.reset_index(drop=True, inplace=True)\n",
    "print('Dataset A')\n",
    "dataA.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dropping records from both Dataset A and B that were taken from before 2020 and beyond 2023.\n",
    "print(\"dataA records before restriction:\", dataA.shape)\n",
    "dataA = dataA[(dataA['year'] >= 2020) & (dataA['year'] <= 2022)]\n",
    "print(\"dataA records after restriction:\", dataA.shape)\n",
    "dataA.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Possible Imputation to estimate NA values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Deciding whether to drop or impute null values, so we check how many null values there are.\n",
    "# Dataset A impute\n",
    "print(\"A: Number of entries with null values:\", dataA.isna().any(axis=1).sum())\n",
    "print(\"A: Number of entries:\", dataA.shape[0])\n",
    "\n",
    "# Dataset B impute\n",
    "print(\"B: Number of entries with null values:\", dataB.isna().any(axis=1).sum())\n",
    "print(\"B: Number of entries:\", dataB.shape[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see for Dataset A, we have 6741 entries with null values out of a total of 6872 entries. Hence, dropping those entries is similar to just dropping the entire dataset. Thus, imputation must be performed to preserve Dataset A.\n",
    "\n",
    "As for Dataset B, we find that there are 0 entries with null values, hence we can work with it as it is.\n",
    "\n",
    "**Decision:** We apply multivariate imputation to Dataset A, while Dataset B will remain the same.\n",
    "\n",
    "However, further examination of the dataset reveals that there are countries where almost the entire column is populated with NAs. Hence, for better reliability â€” that is, not making up values from thin air â€” we will perform imputation per country, and then all together.\n",
    "\n",
    "We will now do **Per Country Imputation of Dataset A**."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# These imports are important, imputer relies on them.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer   # Important!\n",
    "from sklearn.impute import IterativeImputer     # default imputer is BayesianRidge\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "print(\"Before imputation:\")\n",
    "dataA\n",
    "\n",
    "countries = dataA['country'].unique()\n",
    "\n",
    "# Initialize imputer\n",
    "imp = IterativeImputer(max_iter=100, random_state=1)\n",
    "\n",
    "# Idea: Drop country and year from a copy of Dataset A, then\n",
    "# make that accumulate imputed values, then restore the dropped columns\n",
    "# Assumes country and year are the first 2 columns of Dataset A\n",
    "accumulator = []    # List of rows\n",
    "for country in countries:\n",
    "    temp = dataA[dataA['country'] == country].drop(['country', 'year'], axis=1)\n",
    "    imputed = imp.fit_transform(temp)\n",
    "    if imputed.shape[1] != 6:\n",
    "        # Failure of imputation (i.e. not getting all 6 attributes imputed) would mean that the country could be\n",
    "        # dropped from the dataset, as the failure implies too many NaN values for the records to be sensibly used\n",
    "        print(\"{} ({})\".format(country, \"Dropped\"))\n",
    "        dataA.drop(dataA[dataA.country == country].index, inplace=True)\n",
    "        continue\n",
    "    else:\n",
    "        print(country)\n",
    "        accumulator.extend(imputed)\n",
    "\n",
    "print(\"After imputation:\")\n",
    "cols = dataA[['country', 'year']].copy()\n",
    "dataA.drop(['country', 'year'], axis=1, inplace=True)   # Dropping for shape compatibility\n",
    "dataA[:] = accumulator\n",
    "# Restoring dropped columns\n",
    "dataA.insert(0, 'country', cols.country.values)\n",
    "dataA.insert(1, 'year', cols.year.values)\n",
    "dataA.reset_index(drop=True, inplace=True)\n",
    "dataA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking if there are still NAN values in dataset A\n",
    "print(\"A: Number of entries with null values:\", dataA.isna().any(axis=1).sum())\n",
    "print(\"A: Number of entries:\", dataA.shape[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Merging Dataset A with Dataset B\n",
    "Now that Dataset A has been successfully preprocessed, we can then merge it with Dataset B.\n",
    "We will apply Linear Regression on the Result."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show again what we're dealing with\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
